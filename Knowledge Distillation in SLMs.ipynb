{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport torch.optim as optim\nimport math\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:07:30.938634Z","iopub.execute_input":"2025-07-28T17:07:30.939342Z","iopub.status.idle":"2025-07-28T17:07:30.942970Z","shell.execute_reply.started":"2025-07-28T17:07:30.939315Z","shell.execute_reply":"2025-07-28T17:07:30.942386Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:07:30.945056Z","iopub.execute_input":"2025-07-28T17:07:30.945277Z","iopub.status.idle":"2025-07-28T17:07:30.961216Z","shell.execute_reply.started":"2025-07-28T17:07:30.945260Z","shell.execute_reply":"2025-07-28T17:07:30.960628Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# **Load Dataset**","metadata":{}},{"cell_type":"markdown","source":"**text:** a string feature.\n\n**label:** a classification label, with possible values including World (0), Sports (1), Business (2), Sci/Tech (3).\n\n**train:** 120000\n\n**test:** 7600","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the AG News dataset from Hugging Face Hub\ndataset = load_dataset(\"wangrongsheng/ag_news\")\n\n# Display the first two training samples\nprint(\"Sample 1:\", dataset[\"train\"][0])\nprint(\"Sample 2:\", dataset[\"train\"][1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:07:30.962248Z","iopub.execute_input":"2025-07-28T17:07:30.962488Z","iopub.status.idle":"2025-07-28T17:07:32.369659Z","shell.execute_reply.started":"2025-07-28T17:07:30.962467Z","shell.execute_reply":"2025-07-28T17:07:32.368981Z"}},"outputs":[{"name":"stdout","text":"Sample 1: {'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\nSample 2: {'text': 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.', 'label': 2}\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"# **Tokenization**","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer (uncased version)\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Define the maximum sequence length\nmax_len = 128\n\n# Tokenization function for a batch of text samples\ndef tokenize_batch(dataset_split):\n    return tokenizer(\n        [sample[\"text\"] for sample in dataset_split],  # Extract texts\n        padding=\"max_length\",                          # Pad to max_len\n        truncation=True,                               # Truncate if too long\n        max_length=max_len,\n        return_tensors=\"pt\"                            # Return PyTorch tensors\n    )\n\n# Apply tokenization to train and test sets\ntrain_encodings = tokenize_batch(dataset[\"train\"])\ntest_encodings = tokenize_batch(dataset[\"test\"])\n\n# Extract labels and convert them to PyTorch tensors\ntrain_labels = torch.tensor([sample[\"label\"] for sample in dataset[\"train\"]])\ntest_labels = torch.tensor([sample[\"label\"] for sample in dataset[\"test\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:07:32.370863Z","iopub.execute_input":"2025-07-28T17:07:32.371050Z","iopub.status.idle":"2025-07-28T17:09:13.545498Z","shell.execute_reply.started":"2025-07-28T17:07:32.371035Z","shell.execute_reply":"2025-07-28T17:09:13.544872Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"print(train_encodings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.546229Z","iopub.execute_input":"2025-07-28T17:09:13.546468Z","iopub.status.idle":"2025-07-28T17:09:13.553260Z","shell.execute_reply.started":"2025-07-28T17:09:13.546444Z","shell.execute_reply":"2025-07-28T17:09:13.552419Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  101,  2813,  2358,  ...,     0,     0,     0],\n        [  101, 18431,  2571,  ...,     0,     0,     0],\n        [  101,  3514,  1998,  ...,     0,     0,     0],\n        ...,\n        [  101,  7842,  8193,  ...,     0,     0,     0],\n        [  101,  2651,  1005,  ...,  2038,  2589,   102],\n        [  101, 16996,  2131,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0]])}\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"# **Create PyTorch Dataset**","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n# Define a custom dataset class for AG News\nclass AGNewsDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': self.labels[idx]\n        }\n\n# Create dataset instances\ntrain_dataset = AGNewsDataset(train_encodings, train_labels)\ntest_dataset = AGNewsDataset(test_encodings, test_labels)\n\n# Define batch size\nbatch_size = 32\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.555019Z","iopub.execute_input":"2025-07-28T17:09:13.555265Z","iopub.status.idle":"2025-07-28T17:09:13.603318Z","shell.execute_reply.started":"2025-07-28T17:09:13.555247Z","shell.execute_reply":"2025-07-28T17:09:13.602779Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"# **Teacher Model**","metadata":{}},{"cell_type":"markdown","source":"![](https://heidloff.net/assets/img/2023/02/transformers.png)?","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, hidden_dim, num_heads, ff_dim):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n        self.attn_norm = nn.LayerNorm(hidden_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(hidden_dim, ff_dim),\n            nn.ReLU(),\n            nn.Linear(ff_dim, hidden_dim)\n        )\n        self.ff_norm = nn.LayerNorm(hidden_dim)\n\n    def forward(self, x, attention_mask=None):\n        # Create key padding mask from attention mask\n        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n\n        # Multi-head self-attention\n        attn_output, attn_weights = self.attention(\n            x, x, x,\n            key_padding_mask=key_padding_mask,\n            need_weights=True,\n            average_attn_weights=False\n        )\n        value_vectors = x  # Input to attention acts as Value\n\n        # Add & Norm (post-attention)\n        x = self.attn_norm(x + attn_output)\n\n        # Feed-forward network with Add & Norm\n        ff_output = self.ff(x)\n        x = self.ff_norm(x + ff_output)\n\n        return x, attn_weights, value_vectors\n\n\nclass TeacherTransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, hidden_dim=384, num_heads=8, ff_dim=1536, max_len=128, num_layers=4, type_vocab_size=2):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n\n        # Embedding layers: token + position + segment (type)\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_embedding = nn.Embedding(max_len, hidden_dim)\n        self.segment_embedding = nn.Embedding(type_vocab_size, hidden_dim)\n\n        # Transformer blocks\n        self.layers = nn.ModuleList([\n            TransformerBlock(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)\n        ])\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n        # input_ids.shape = torch.Size([32, 128])\n        B, L = input_ids.shape\n        device = input_ids.device\n\n        # Create position indices\n        positions = torch.arange(0, L, device=device).unsqueeze(0).expand(B, L)\n\n        # If segment (token type) IDs are not provided, default to all zeros (segment A)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # Combine embeddings: token + position + segment\n        x = self.embedding(input_ids) \\\n            + self.pos_embedding(positions) \\\n            + self.segment_embedding(token_type_ids)\n\n        # Forward through transformer layers\n        attentions = None\n        value_vectors = None\n\n        for i, layer in enumerate(self.layers):\n            x, attn_weights, values = layer(x, attention_mask)\n\n            # Save only final layer's outputs for distillation\n            if i == len(self.layers) - 1:\n                attentions = attn_weights\n                value_vectors = values\n                \n        return {\n            \"last_hidden_state\": x,\n            \"attentions\": attentions,\n            \"value_vectors\": value_vectors\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.604014Z","iopub.execute_input":"2025-07-28T17:09:13.604212Z","iopub.status.idle":"2025-07-28T17:09:13.614070Z","shell.execute_reply.started":"2025-07-28T17:09:13.604197Z","shell.execute_reply":"2025-07-28T17:09:13.613382Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# Instantiate the teacher model and move it to the appropriate device (CPU or GPU)\nteacher_model = TeacherTransformerEncoder(vocab_size=tokenizer.vocab_size).to(device)\n\n# Fetch one batch of data from the training loader\nbatch = next(iter(train_loader))\n\n# Move input tensors to the same device as the model\ninput_ids = batch['input_ids'].to(device)\nattention_mask = batch['attention_mask'].to(device)\n\n# Forward pass through the teacher model\nout_t = teacher_model(input_ids, attention_mask)\n\n# Print the shape of attention outputs (should be B x num_heads x L x L)\nprint(\"Teacher attention output shape: \", out_t[\"attentions\"].shape)\n\n# Print the shape of value vectors from the last transformer layer\nprint(\"Teacher value output shape: \", out_t[\"value_vectors\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.614732Z","iopub.execute_input":"2025-07-28T17:09:13.614924Z","iopub.status.idle":"2025-07-28T17:09:13.821470Z","shell.execute_reply.started":"2025-07-28T17:09:13.614910Z","shell.execute_reply":"2025-07-28T17:09:13.820773Z"}},"outputs":[{"name":"stdout","text":"Teacher attention output shape:  torch.Size([32, 8, 100, 100])\nTeacher value output shape:  torch.Size([32, 100, 384])\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"# **Student Model**\n","metadata":{}},{"cell_type":"code","source":"class StudentTransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, hidden_dim=128, num_heads=8, ff_dim=256, max_len=128, num_classes=4, type_vocab_size=2):\n        super(StudentTransformerEncoder, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n\n        # Embedding layers: token, position, and segment (as in BERT)\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_embedding = nn.Embedding(max_len, hidden_dim)\n        self.segment_embedding = nn.Embedding(type_vocab_size, hidden_dim)\n\n        # Self-attention block\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim, \n            num_heads=num_heads, \n            batch_first=True\n        )\n        self.attn_norm = nn.LayerNorm(hidden_dim)\n\n        # Feed-forward block\n        self.ff = nn.Sequential(\n            nn.Linear(hidden_dim, ff_dim),\n            nn.ReLU(),\n            nn.Linear(ff_dim, hidden_dim)\n        )\n        self.ff_norm = nn.LayerNorm(hidden_dim)\n\n        # Optional classification head\n        if num_classes is not None:\n            self.classifier = nn.Linear(hidden_dim, num_classes)\n        else:\n            self.classifier = None\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n        B, L = input_ids.size()\n        device = input_ids.device\n\n        # Positional embedding\n        positions = torch.arange(0, L, device=device).unsqueeze(0).expand(B, L)\n\n        # Default to all zeros (segment A) if token_type_ids not provided\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # Combine embeddings: token + position + segment\n        x = self.embedding(input_ids) \\\n            + self.pos_embedding(positions) \\\n            + self.segment_embedding(token_type_ids)\n\n        # Attention masking (PAD = 0 → True)\n        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n\n        # Self-attention with attention weights\n        attn_output, attn_weights = self.attention(\n            x, x, x,\n            key_padding_mask=key_padding_mask,\n            need_weights=True,\n            average_attn_weights=False\n        )\n\n         # Save pre-attention input as value vectors (for VR loss)\n        value_vectors = x.detach()  # (B, L, D)\n\n        # Residual + LayerNorm after attention\n        x = self.attn_norm(x + attn_output)\n\n        # Feed-forward + Residual + LayerNorm\n        ff_output = self.ff(x)\n        x = self.ff_norm(x + ff_output)\n\n        # Prepare outputs\n        output = {\n            \"last_hidden_state\": x,         # (B, L, D)\n            \"attentions\": attn_weights,     # (B, num_heads, L, L)\n            \"value_vectors\": value_vectors  # (B, L, D)\n        }\n\n        # Add classification logits if classifier is defined\n        if self.classifier is not None:\n            cls_repr = x[:, 0, :]  # First token (CLS)\n            output[\"logits\"] = self.classifier(cls_repr)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.822207Z","iopub.execute_input":"2025-07-28T17:09:13.822472Z","iopub.status.idle":"2025-07-28T17:09:13.830930Z","shell.execute_reply.started":"2025-07-28T17:09:13.822455Z","shell.execute_reply":"2025-07-28T17:09:13.830402Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# Instantiate the student model and move it to the appropriate device (CPU or GPU)\nstudent_model = StudentTransformerEncoder(vocab_size=tokenizer.vocab_size).to(device)\n\n# Fetch one batch of data from the training loader\nbatch = next(iter(train_loader))\n\n# Move input tensors to the same device as the model\ninput_ids = batch['input_ids'].to(device)\nattention_mask = batch['attention_mask'].to(device)\n\n# Forward pass through the teacher model\nout_s = student_model(input_ids, attention_mask)\n\n# Print the shape of attention outputs (should be B x num_heads x L x L)\nprint(\"Teacher attention output shape: \", out_s[\"attentions\"].shape)\n\n# Print the shape of value vectors from the last transformer layer\nprint(\"Teacher value output shape: \", out_s[\"value_vectors\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.831569Z","iopub.execute_input":"2025-07-28T17:09:13.831768Z","iopub.status.idle":"2025-07-28T17:09:13.897732Z","shell.execute_reply.started":"2025-07-28T17:09:13.831753Z","shell.execute_reply":"2025-07-28T17:09:13.897131Z"}},"outputs":[{"name":"stdout","text":"Teacher attention output shape:  torch.Size([32, 8, 100, 100])\nTeacher value output shape:  torch.Size([32, 100, 128])\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"# **Loss Function**","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.898354Z","iopub.execute_input":"2025-07-28T17:09:13.898569Z","iopub.status.idle":"2025-07-28T17:09:13.901586Z","shell.execute_reply.started":"2025-07-28T17:09:13.898553Z","shell.execute_reply":"2025-07-28T17:09:13.900933Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"def attention_loss(student_attn_last, teacher_attn_last, eps=1e-8):\n    \"\"\"\n    Compute KL-divergence loss between teacher and student attention distributions.\n    \n    Args:\n        student_attn_last: Tensor of shape (B, A_h, L, L) - student's attention scores.\n        teacher_attn_last: Tensor of shape (B, A_h, L, L) - teacher's attention scores.\n        eps: small constant for numerical stability.\n\n    Returns:\n        Scalar tensor representing the averaged KL divergence loss.\n    \"\"\"\n    # Clamp values to avoid log(0)\n    teacher_attn = teacher_attn_last.clamp(min=eps)\n    student_attn = student_attn_last.clamp(min=eps)\n\n    # Compute element-wise KL divergence: KL(teacher || student)\n    kl = teacher_attn * (torch.log(teacher_attn) - torch.log(student_attn))\n    \n    # Sum over keys dimension (last dimension)\n    kl = kl.sum(dim=-1)  # shape: (B, A_h, L)\n    \n    # Average over batch, attention heads, and tokens\n    loss = kl.mean()\n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.903431Z","iopub.execute_input":"2025-07-28T17:09:13.903597Z","iopub.status.idle":"2025-07-28T17:09:13.920136Z","shell.execute_reply.started":"2025-07-28T17:09:13.903584Z","shell.execute_reply":"2025-07-28T17:09:13.919618Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"def value_relation_loss(student_value_last, teacher_value_last, eps=1e-8):\n    \"\"\"\n    Compute KL-divergence loss between value relation matrices of teacher and student.\n    \n    Args:\n        student_value_last: Tensor of shape (B, L, D) - student's value vectors.\n        teacher_value_last: Tensor of shape (B, L, D) - teacher's value vectors.\n        eps: small constant for numerical stability.\n\n    Returns:\n        Scalar tensor representing the averaged KL divergence loss.\n    \"\"\"\n    # Normalize value vectors along feature dimension\n    sv = F.normalize(student_value_last, p=2, dim=-1)  # (B, L, D)\n    tv = F.normalize(teacher_value_last, p=2, dim=-1)  # (B, L, D)\n\n    # Compute relation matrices: similarity between tokens\n    student_rel = torch.matmul(sv, sv.transpose(-1, -2))  # (B, L, L)\n    teacher_rel = torch.matmul(tv, tv.transpose(-1, -2))  # (B, L, L)\n\n    # sv.shape == (B, L, D)\n    # sv.transpose(-1, -2) shape = (B, D, L)\n    # student_rel.shape => (B, L, D) @ (B, D, L) → (B, L, L)\n\n    # Convert to probability distributions using softmax along keys dimension\n    student_rel = F.softmax(student_rel, dim=-1).clamp(min=eps)\n    teacher_rel = F.softmax(teacher_rel, dim=-1).clamp(min=eps)\n\n    # Compute element-wise KL divergence: KL(teacher || student)\n    kl = teacher_rel * (torch.log(teacher_rel) - torch.log(student_rel))\n\n    # Sum over keys dimension\n    kl = kl.sum(dim=-1)  # shape: (B, L)\n\n    # Average over batch and tokens\n    loss = kl.mean()\n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.920830Z","iopub.execute_input":"2025-07-28T17:09:13.921065Z","iopub.status.idle":"2025-07-28T17:09:13.957932Z","shell.execute_reply.started":"2025-07-28T17:09:13.921043Z","shell.execute_reply":"2025-07-28T17:09:13.957458Z"}},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"# **Forward Pass**","metadata":{}},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss\n\nnum_epochs = 10\noptimizer = optim.Adam(student_model.parameters(), lr=1e-4)\ncriterion = CrossEntropyLoss()\n\nfor epoch in range(num_epochs):\n    print(f\"------------------------ Epoch {epoch+1}/{num_epochs} ------------------------\")\n    \n    student_model.train()\n    teacher_model.eval()\n    train_loss = 0.0\n\n    for batch in tqdm(train_loader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        # ---- TEACHER FORWARD PASS (No gradient) ----\n        with torch.no_grad():\n            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n            teacher_attn = teacher_outputs[\"attentions\"]        # (B, num_heads, L, L)\n            teacher_value = teacher_outputs[\"value_vectors\"]    # (B, L, D)\n\n        # ---- STUDENT FORWARD PASS ----\n        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n        student_attn = student_outputs[\"attentions\"]           # (B, num_heads, L, L)\n        student_value = student_outputs[\"value_vectors\"]       # (B, L, D)\n\n        # ---- DISTILLATION LOSS COMPUTATION ----\n        loss_at = attention_loss(student_attn, teacher_attn)\n        loss_vr = value_relation_loss(student_value, teacher_value)\n        loss = loss_at + loss_vr\n\n        # ---- OPTIMIZATION ----\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    print(f\"Training - Distillation Loss: {train_loss / len(train_loader):.4f}\")\n\n    # print(\"------------------------ Validation ------------------------\")\n    \n    student_model.eval()\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs[\"logits\"]\n            \n            # Classification loss for evaluation\n            loss = criterion(logits, labels)\n            val_loss += loss.item()\n\n    print(f\"Average Validation Loss: {val_loss / len(test_loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:13.958520Z","iopub.execute_input":"2025-07-28T17:09:13.958689Z","iopub.status.idle":"2025-07-28T17:23:03.663980Z","shell.execute_reply.started":"2025-07-28T17:09:13.958676Z","shell.execute_reply":"2025-07-28T17:23:03.663397Z"}},"outputs":[{"name":"stdout","text":"------------------------ Epoch 1/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0771\nAverage Validation Loss: 1.5559\n------------------------ Epoch 2/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0342\nAverage Validation Loss: 1.5442\n------------------------ Epoch 3/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0280\nAverage Validation Loss: 1.5231\n------------------------ Epoch 4/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0244\nAverage Validation Loss: 1.5079\n------------------------ Epoch 5/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0220\nAverage Validation Loss: 1.4960\n------------------------ Epoch 6/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0204\nAverage Validation Loss: 1.4878\n------------------------ Epoch 7/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0193\nAverage Validation Loss: 1.4823\n------------------------ Epoch 8/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0185\nAverage Validation Loss: 1.4782\n------------------------ Epoch 9/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0179\nAverage Validation Loss: 1.4747\n------------------------ Epoch 10/10 ------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [01:22<00:00, 45.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training - Distillation Loss: 0.0174\nAverage Validation Loss: 1.4718\n","output_type":"stream"}],"execution_count":68}]}